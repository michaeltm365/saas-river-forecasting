{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    classification_report,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "F0RBneetkn5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style=\"whitegrid\", context=\"talk\")\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "EPS = 1e-6  # small constant to avoid divide-by-zero issues"
      ],
      "metadata": {
        "id": "vk-fQpsbkppK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape observational + static data\n",
        "# Observations and static variables are stored in long format, so we pivot them wide to get one row per site-date.\n",
        "\n",
        "obs_wide = (\n",
        "    obs\n",
        "    .pivot_table(\n",
        "        index=[\"NHDPlusID\", \"Date\"],\n",
        "        columns=\"variable\",\n",
        "        values=\"value\",\n",
        "        aggfunc=\"first\"\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "obs_wide.columns.name = None\n",
        "\n",
        "static_wide = (\n",
        "    static\n",
        "    .pivot_table(\n",
        "        index=\"NHDPlusID\",\n",
        "        columns=\"variable\",\n",
        "        values=\"value\",\n",
        "        aggfunc=\"first\"\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "static_wide.columns.name = None\n",
        "\n",
        "# Make sure IDs and dates line up cleanly before merging\n",
        "for df in [obs_wide, drivers, static_wide, degrees]:\n",
        "    df[\"NHDPlusID\"] = df[\"NHDPlusID\"].astype(str)\n",
        "\n",
        "obs_wide[\"Date\"] = pd.to_datetime(obs_wide[\"Date\"], errors=\"coerce\")\n",
        "drivers[\"Date\"] = pd.to_datetime(drivers[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "# Pull out month as a simple seasonal feature\n",
        "obs_wide[\"month\"] = obs_wide[\"Date\"].dt.month\n",
        "drivers[\"month\"] = drivers[\"Date\"].dt.month\n",
        "\n",
        "# Keep only rows where the wet/dry label exists\n",
        "obs_wide = obs_wide[obs_wide[\"HoboWetDry0.05\"].notna()].copy()"
      ],
      "metadata": {
        "id": "1AweKaooksQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create lagged climate variables\n",
        "# Lags are computed *within each site* to avoid leakage.\n",
        "# shift(+k) means \"k days in the past\" (true lag).\n",
        "\n",
        "drivers = drivers.sort_values([\"NHDPlusID\", \"Date\"]).copy()\n",
        "\n",
        "LAG_FEATURES = [\"prcp\", \"tmax\", \"tmin\", \"srad\", \"rhmax\", \"rhmin\", \"vp\", \"ws\"]\n",
        "LAG_DAYS = 1  # set to 0 (no lag), 1 (1-day), or 7 (1-week)\n",
        "\n",
        "for col in LAG_FEATURES:\n",
        "    drivers[f\"{col}_lag{LAG_DAYS}\"] = (\n",
        "        drivers\n",
        "        .groupby(\"NHDPlusID\")[col]\n",
        "        .shift(LAG_DAYS)\n",
        "    )\n"
      ],
      "metadata": {
        "id": "iRX9UNJEk1ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge everything into one table\n",
        "# Inner joins ensure we only keep rows where all required information is available.\n",
        "\n",
        "merged = (\n",
        "    obs_wide\n",
        "    .merge(drivers, on=[\"NHDPlusID\", \"Date\"], how=\"inner\")\n",
        "    .merge(static_wide, on=\"NHDPlusID\", how=\"inner\")\n",
        "    .merge(degrees, on=\"NHDPlusID\", how=\"inner\")\n",
        ")\n",
        "\n",
        "print(\"Merged shape:\", merged.shape)"
      ],
      "metadata": {
        "id": "-raFc_zhk8ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature engineering\n",
        "# These features try to relate dynamic signals (e.g. precip, temp) to static watershed properties rather than letting the model memorize static indicators alone.\n",
        "\n",
        "if \"AreaSqKm\" in merged.columns:\n",
        "    merged[\"prcp_per_area\"] = merged[\"prcp\"] / (merged[\"AreaSqKm\"] + EPS)\n",
        "    merged[\"discharge_per_area\"] = merged[\"Discharge_CMS\"] / (merged[\"AreaSqKm\"] + EPS)\n",
        "\n",
        "if \"Slope\" in merged.columns:\n",
        "    merged[\"tmax_times_slope\"] = merged[\"tmax\"] * merged[\"Slope\"]\n",
        "    merged[\"tmin_times_slope\"] = merged[\"tmin\"] * merged[\"Slope\"]\n",
        "\n",
        "if \"LengthKM\" in merged.columns:\n",
        "    merged[\"flow_per_length\"] = merged[\"Discharge_CMS\"] / (merged[\"LengthKM\"] + EPS)\n",
        "\n",
        "if \"elev_mean_cm\" in merged.columns:\n",
        "    merged[\"temp_minus_elev\"] = merged[\"tmax\"] - (merged[\"elev_mean_cm\"] / 100)"
      ],
      "metadata": {
        "id": "-lbh7Jx_lBtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build feature matrix and target\n",
        "\n",
        "TARGET_COL = \"HoboWetDry0.05\"\n",
        "META_COLS = [\"NHDPlusID\", \"Date\", \"Flow_Status\"]\n",
        "\n",
        "drop_cols = [TARGET_COL] + [c for c in META_COLS if c in merged.columns]\n",
        "\n",
        "X = merged.drop(columns=drop_cols, errors=\"ignore\")\n",
        "X = X.select_dtypes(include=[np.number])\n",
        "y = merged[TARGET_COL].astype(int)\n",
        "\n",
        "# Clean up any weird values before modeling\n",
        "X = X.replace([np.inf, -np.inf], np.nan)\n",
        "X = X.fillna(X.mean()).fillna(0)\n",
        "\n",
        "print(\"Feature matrix shape:\", X.shape)\n",
        "print(\"Label distribution:\", y.value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "uas22PaSlOi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Temporal train/test split\n",
        "\n",
        "# We split by date to mimic a real forecasting scenario:\n",
        "# train on earlier data, test on later data.\n",
        "\n",
        "merged_sorted = merged.sort_values(\"Date\")\n",
        "cutoff = int(len(merged_sorted) * 0.8)\n",
        "\n",
        "train_df = merged_sorted.iloc[:cutoff]\n",
        "test_df = merged_sorted.iloc[cutoff:]\n",
        "\n",
        "X_train = train_df.drop(columns=drop_cols, errors=\"ignore\").select_dtypes(include=[np.number])\n",
        "X_test  = test_df.drop(columns=drop_cols, errors=\"ignore\").select_dtypes(include=[np.number])\n",
        "\n",
        "y_train = train_df[TARGET_COL].astype(int)\n",
        "y_test  = test_df[TARGET_COL].astype(int)\n",
        "\n",
        "# Fill missing values using training statistics only\n",
        "X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(X_train.mean()).fillna(0)\n",
        "X_test  = X_test.replace([np.inf, -np.inf], np.nan).fillna(X_train.mean()).fillna(0)\n",
        "\n",
        "print(\"Train dates:\", train_df[\"Date\"].min(), \"→\", train_df[\"Date\"].max())\n",
        "print(\"Test dates:\", test_df[\"Date\"].min(), \"→\", test_df[\"Date\"].max())\n"
      ],
      "metadata": {
        "id": "wRROL-GclQd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale features\n",
        "# Standardization helps LR converge and keeps coefficients comparable.\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "I6Cb_7xylVEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle class imbalance with SMOTE\n",
        "# SMOTE is applied *only to the training set*.\n",
        "\n",
        "print(\"Before SMOTE:\", np.bincount(y_train))\n",
        "smote = SMOTE(random_state=RANDOM_STATE, k_neighbors=5)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
        "print(\"After SMOTE:\", np.bincount(y_train_smote))\n"
      ],
      "metadata": {
        "id": "Jd5-iiOGlYcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train + evaluate logistic regression\n",
        "\n",
        "model = LogisticRegression(max_iter=3000, random_state=RANDOM_STATE)\n",
        "model.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# AUC can be undefined if the test set has only one class\n",
        "auc = np.nan\n",
        "if len(np.unique(y_test)) == 2:\n",
        "    auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(f\"\\nAccuracy: {acc:.3f}\")\n",
        "print(f\"F1 Score: {f1:.3f}\")\n",
        "print(f\"AUC: {auc:.3f}\" if not np.isnan(auc) else \"AUC: nan\")\n",
        "\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "# Confusion matrix for quick sanity check\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    cmap=\"Blues\",\n",
        "    xticklabels=[\"Pred Dry\", \"Pred Wet\"],\n",
        "    yticklabels=[\"True Dry\", \"True Wet\"]\n",
        ")\n",
        "plt.title(\"Confusion Matrix (Temporal Split)\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dJRQiUpNlcC-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}