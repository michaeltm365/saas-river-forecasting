{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37418adf",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d140d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import optuna\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954c3c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_vars_df head:\n",
      "        NHDPlusID       variable          value\n",
      "0  55000900028341  aspect_ne_pct       0.074821\n",
      "1  55000900028341  aspect_sw_pct       0.007807\n",
      "2  55000900028341  aspect_nw_pct       0.917372\n",
      "3  55000900028341  aspect_se_pct       0.000000\n",
      "4  55000900028341    elev_min_cm  103297.000000\n",
      "\n",
      "obs_df head:\n",
      "        NHDPlusID SiteIDCode        Date       variable     value\n",
      "0  55000900130309     GSWS01  1980-01-01  Discharge_CMS  0.045505\n",
      "1  55000900130309     GSWS01  1980-01-02  Discharge_CMS  0.043410\n",
      "2  55000900130309     GSWS01  1980-01-03  Discharge_CMS  0.048592\n",
      "3  55000900130309     GSWS01  1980-01-04  Discharge_CMS  0.061872\n",
      "4  55000900130309     GSWS01  1980-01-05  Discharge_CMS  0.276599\n"
     ]
    }
   ],
   "source": [
    "static_vars_df = pd.read_parquet('static_vars.parquet')\n",
    "obs_df = pd.read_parquet('obs.parquet')\n",
    "\n",
    "print('static_vars_df head:')\n",
    "print(static_vars_df.head())\n",
    "print('\\nobs_df head:')\n",
    "print(obs_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914766fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "merged_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-e6fbd384-48cc-4dc4-be1d-c0b293cbe9da\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variable</th>\n",
       "      <th>NHDPlusID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Discharge_CMS</th>\n",
       "      <th>Flow_Status</th>\n",
       "      <th>HoboWetDry0.05</th>\n",
       "      <th>MaxDepth_Censor</th>\n",
       "      <th>MaxDepth_Threshold</th>\n",
       "      <th>MaxDepth_cm</th>\n",
       "      <th>ArbolateSu</th>\n",
       "      <th>AreaSqKm</th>\n",
       "      <th>...</th>\n",
       "      <th>aspect_se_pct</th>\n",
       "      <th>aspect_sw_pct</th>\n",
       "      <th>curv_mean</th>\n",
       "      <th>curv_median</th>\n",
       "      <th>elev_max_cm</th>\n",
       "      <th>elev_mean_cm</th>\n",
       "      <th>elev_median_cm</th>\n",
       "      <th>elev_min_cm</th>\n",
       "      <th>slp_mean_pct</th>\n",
       "      <th>slp_median_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55000900027171</td>\n",
       "      <td>2020-08-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100818</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>63.946892</td>\n",
       "      <td>63.048618</td>\n",
       "      <td>66223.0</td>\n",
       "      <td>58209.588235</td>\n",
       "      <td>57898.0</td>\n",
       "      <td>51722.0</td>\n",
       "      <td>63.946892</td>\n",
       "      <td>63.048618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55000900027173</td>\n",
       "      <td>2020-08-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.297666</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.565992</td>\n",
       "      <td>42.080586</td>\n",
       "      <td>41.399857</td>\n",
       "      <td>103022.0</td>\n",
       "      <td>88853.689879</td>\n",
       "      <td>88575.0</td>\n",
       "      <td>74487.0</td>\n",
       "      <td>42.080586</td>\n",
       "      <td>41.399857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55000900027174</td>\n",
       "      <td>2020-08-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.410513</td>\n",
       "      <td>0.0923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55.872930</td>\n",
       "      <td>56.605061</td>\n",
       "      <td>96559.0</td>\n",
       "      <td>79958.395450</td>\n",
       "      <td>80669.0</td>\n",
       "      <td>63278.0</td>\n",
       "      <td>55.872930</td>\n",
       "      <td>56.605061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55000900027177</td>\n",
       "      <td>2020-09-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.366996</td>\n",
       "      <td>0.1216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.079770</td>\n",
       "      <td>63.409000</td>\n",
       "      <td>65.848724</td>\n",
       "      <td>72273.0</td>\n",
       "      <td>60448.368421</td>\n",
       "      <td>60738.5</td>\n",
       "      <td>46037.0</td>\n",
       "      <td>63.409000</td>\n",
       "      <td>65.848724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55000900027180</td>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.082607</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.379627</td>\n",
       "      <td>22.186161</td>\n",
       "      <td>55631.0</td>\n",
       "      <td>49135.785473</td>\n",
       "      <td>48683.0</td>\n",
       "      <td>46107.0</td>\n",
       "      <td>21.379627</td>\n",
       "      <td>22.186161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e6fbd384-48cc-4dc4-be1d-c0b293cbe9da')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-e6fbd384-48cc-4dc4-be1d-c0b293cbe9da button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-e6fbd384-48cc-4dc4-be1d-c0b293cbe9da');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-4bdb906d-89c5-4f35-952b-91a7329988a3\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4bdb906d-89c5-4f35-952b-91a7329988a3')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-4bdb906d-89c5-4f35-952b-91a7329988a3 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "variable       NHDPlusID        Date  Discharge_CMS  Flow_Status  \\\n",
       "0         55000900027171  2020-08-12            NaN         0.00   \n",
       "1         55000900027173  2020-08-12            NaN         0.75   \n",
       "2         55000900027174  2020-08-12            NaN         1.00   \n",
       "3         55000900027177  2020-09-03            NaN         0.50   \n",
       "4         55000900027180  2020-09-02            NaN         0.50   \n",
       "\n",
       "variable  HoboWetDry0.05  MaxDepth_Censor  MaxDepth_Threshold  MaxDepth_cm  \\\n",
       "0                    NaN              0.0                 0.0          0.0   \n",
       "1                    NaN              1.0                 0.0          7.0   \n",
       "2                    NaN              1.0                 0.0          5.5   \n",
       "3                    NaN              1.0                 0.0          1.0   \n",
       "4                    NaN              1.0                 0.0         11.0   \n",
       "\n",
       "variable  ArbolateSu  AreaSqKm  ...  aspect_se_pct  aspect_sw_pct  curv_mean  \\\n",
       "0           0.100818    0.0017  ...         0.0000       1.000000  63.946892   \n",
       "1           0.297666    0.1235  ...         0.0000       0.565992  42.080586   \n",
       "2           0.410513    0.0923  ...         0.0000       0.000000  55.872930   \n",
       "3           0.366996    0.1216  ...         0.6875       0.079770  63.409000   \n",
       "4           1.082607    0.0592  ...         0.0000       0.000000  21.379627   \n",
       "\n",
       "variable  curv_median  elev_max_cm  elev_mean_cm  elev_median_cm  elev_min_cm  \\\n",
       "0           63.048618      66223.0  58209.588235         57898.0      51722.0   \n",
       "1           41.399857     103022.0  88853.689879         88575.0      74487.0   \n",
       "2           56.605061      96559.0  79958.395450         80669.0      63278.0   \n",
       "3           65.848724      72273.0  60448.368421         60738.5      46037.0   \n",
       "4           22.186161      55631.0  49135.785473         48683.0      46107.0   \n",
       "\n",
       "variable  slp_mean_pct  slp_median_pct  \n",
       "0            63.946892       63.048618  \n",
       "1            42.080586       41.399857  \n",
       "2            55.872930       56.605061  \n",
       "3            63.409000       65.848724  \n",
       "4            21.379627       22.186161  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pivot static vars (each variable as a column)\n",
    "static_wide = static_vars_df.pivot(index='NHDPlusID', columns='variable', values='value').reset_index()\n",
    "\n",
    "# Pivot obs_df (Date as rows, variable as columns)\n",
    "obs_wide = obs_df.pivot_table(index=['NHDPlusID', 'Date'], columns='variable', values='value').reset_index()\n",
    "\n",
    "# Merge static features into each site’s obs\n",
    "merged_df = obs_wide.merge(static_wide, on='NHDPlusID', how='left')\n",
    "\n",
    "# Sort and forward-fill\n",
    "merged_df = merged_df.sort_values(['NHDPlusID', 'Date']).ffill()\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34063fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'HoboWetDry0.05'\n",
    "feature_cols = [c for c in merged_df.columns if c not in ['Date', 'NHDPlusID', target_col]]\n",
    "\n",
    "# Normalize numeric cols\n",
    "scaler = StandardScaler()\n",
    "merged_df[feature_cols] = scaler.fit_transform(merged_df[feature_cols])\n",
    "\n",
    "# Create sequences (e.g., 30-day window)\n",
    "def create_sequences(df, seq_len=30):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - seq_len):\n",
    "        seq_x = df.iloc[i:i+seq_len][feature_cols].values\n",
    "        seq_y = df.iloc[i+seq_len][target_col]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Example: use one NHDPlusID for now\n",
    "site_df = merged_df[merged_df['NHDPlusID'] == merged_df['NHDPlusID'].unique()[0]].dropna()\n",
    "X, y = create_sequences(site_df, seq_len=30)\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "dataset = TensorDataset(X_tensor, y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec906ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using site 55000900061097 with 15883 valid samples\n",
      "Generated 15853 sequences with seq_len=30\n",
      "Tensor shapes: torch.Size([15853, 30, 24]) torch.Size([15853, 1])\n"
     ]
    }
   ],
   "source": [
    "# pick the site with most valid HoboWetDry0.05 values\n",
    "site_counts = merged_df.groupby(\"NHDPlusID\")[\"HoboWetDry0.05\"].apply(lambda x: x.dropna().shape[0])\n",
    "best_site = site_counts.idxmax()\n",
    "print(f\"Using site {best_site} with {site_counts[best_site]} valid samples\")\n",
    "\n",
    "# subset data for that site\n",
    "site_df = merged_df[merged_df[\"NHDPlusID\"] == best_site].dropna(subset=[\"HoboWetDry0.05\"]).sort_values(\"Date\")\n",
    "\n",
    "# shorter sequence length to ensure dense training data\n",
    "seq_len = 30  # you can raise later (e.g., 60 or 90)\n",
    "\n",
    "X, y = create_sequences(site_df, seq_len=seq_len)\n",
    "print(f\"Generated {len(X)} sequences with seq_len={seq_len}\")\n",
    "\n",
    "# convert to tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "print(\"Tensor shapes:\", X_tensor.shape, y_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaba375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTUNA SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7d7357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.45)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
      "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: colorlog, optuna\n",
      "Successfully installed colorlog-6.10.1 optuna-4.6.0\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f4771f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-03 02:03:33,072] A new study created in memory with name: no-name-3d94b82d-7690-4eae-b362-f0139b93ca95\n",
      "[I 2026-01-03 02:05:41,832] Trial 0 finished with value: 1.0 and parameters: {'hidden_size': 110, 'num_layers': 2, 'dropout': 0.1621773443438842, 'lr': 0.0024719059411162374, 'batch_size': 32}. Best is trial 0 with value: 1.0.\n",
      "[I 2026-01-03 02:06:56,454] Trial 1 finished with value: 1.0 and parameters: {'hidden_size': 80, 'num_layers': 2, 'dropout': 0.36362996979722584, 'lr': 0.004156881972742796, 'batch_size': 16}. Best is trial 0 with value: 1.0.\n",
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.14722933912641886 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2026-01-03 02:07:51,729] Trial 2 finished with value: 1.0 and parameters: {'hidden_size': 113, 'num_layers': 1, 'dropout': 0.14722933912641886, 'lr': 0.008587481210770737, 'batch_size': 16}. Best is trial 0 with value: 1.0.\n",
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2925769106904386 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2026-01-03 02:08:30,304] Trial 3 finished with value: 1.0 and parameters: {'hidden_size': 101, 'num_layers': 1, 'dropout': 0.2925769106904386, 'lr': 0.002244037003505455, 'batch_size': 64}. Best is trial 0 with value: 1.0.\n",
      "[I 2026-01-03 02:10:41,033] Trial 4 finished with value: 1.0 and parameters: {'hidden_size': 123, 'num_layers': 2, 'dropout': 0.24554823398566203, 'lr': 0.008273875759385632, 'batch_size': 16}. Best is trial 0 with value: 1.0.\n",
      "[I 2026-01-03 02:11:59,671] Trial 5 finished with value: 1.0 and parameters: {'hidden_size': 81, 'num_layers': 2, 'dropout': 0.08743546269363134, 'lr': 0.002058077810718548, 'batch_size': 16}. Best is trial 0 with value: 1.0.\n",
      "[I 2026-01-03 02:12:40,084] Trial 6 finished with value: 1.0 and parameters: {'hidden_size': 40, 'num_layers': 2, 'dropout': 0.05205034642015876, 'lr': 0.006090597838236091, 'batch_size': 16}. Best is trial 0 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Best Hyperparameters:\n",
      "{'hidden_size': 110, 'num_layers': 2, 'dropout': 0.1621773443438842, 'lr': 0.0024719059411162374, 'batch_size': 32}\n",
      "Best Validation AUC: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# --- Split dataset ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_tensor, y_tensor, test_size=0.2, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "# --- Define LSTM model class ---\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "# --- Define Optuna objective ---\n",
    "def objective(trial):\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 32, 128)\n",
    "    num_layers  = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout     = trial.suggest_float(\"dropout\", 0.0, 0.4)\n",
    "    lr          = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    batch_size  = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "    epochs      = 7 # short tuning run\n",
    "\n",
    "    model = LSTMModel(\n",
    "        input_size=X_tensor.shape[2],\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(TensorDataset(X_val,   y_val),   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # --- training ---\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # --- validation ---\n",
    "    model.eval()\n",
    "    preds, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            p = model(xb).squeeze().cpu().numpy()\n",
    "            preds.append(p)\n",
    "            actuals.append(yb.squeeze().cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    actuals = np.concatenate(actuals)\n",
    "\n",
    "    if len(np.unique(actuals)) < 2:\n",
    "        return 1.0  # meaningless trial\n",
    "\n",
    "    auc = roc_auc_score(actuals, preds)\n",
    "    return 1 - auc  # minimize (1 - AUC)\n",
    "\n",
    "# --- run Optuna study (8 trials) ---\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials = 7)\n",
    "\n",
    "print(\"\\n✅ Best Hyperparameters:\")\n",
    "print(study.best_params)\n",
    "print(f\"Best Validation AUC: {1 - study.best_value:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdc2a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optuna visualizations (to see)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc2f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.visualization import (\n",
    "    plot_optimization_history,\n",
    "    plot_param_importances,\n",
    "    plot_slice,\n",
    "    plot_parallel_coordinate,\n",
    "    plot_contour\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63951093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization.matplotlib import plot_contour\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plot_contour(\n",
    "    study,\n",
    "    params=[\"lr\", \"dropout\"]   # you can swap in any two/three\n",
    ")\n",
    "plt.title(\"Contour Plot: hidden_size vs lr\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67127b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_param_importances(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3209af18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new model with optuna hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a70c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Train/val split ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_tensor, y_tensor, test_size=0.2, shuffle=True, random_state=42\n",
    ")\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape)\n",
    "\n",
    "# --- best params from Optuna ---\n",
    "best_params = {\n",
    "    'hidden_size': 69,\n",
    "    'num_layers': 1,\n",
    "    'dropout': 0.25944456350220574,\n",
    "    'lr': 0.003440467271425508,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "# --- define model ---\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "model = LSTMModel(\n",
    "    input_size=X_tensor.shape[2],\n",
    "    hidden_size=best_params['hidden_size'],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    dropout=best_params['dropout']\n",
    ")\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=best_params['batch_size'], shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(X_val,   y_val),   batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "# --- train for 10 epochs ---\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {total_loss/len(train_loader):.6f}\")\n",
    "\n",
    "# --- get predictions ---\n",
    "def get_preds(loader):\n",
    "    preds, actuals = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            p = model(xb).squeeze().cpu().numpy()\n",
    "            preds.append(p)\n",
    "            actuals.append(yb.squeeze().cpu().numpy())\n",
    "    return np.concatenate(preds), np.concatenate(actuals)\n",
    "\n",
    "train_p, train_y = get_preds(train_loader)\n",
    "val_p,   val_y   = get_preds(val_loader)\n",
    "\n",
    "thr = 0.5\n",
    "train_c = (train_p > thr).astype(int)\n",
    "val_c   = (val_p > thr).astype(int)\n",
    "\n",
    "# --- metrics ---\n",
    "def evaluate(y_true, y_prob, y_pred):\n",
    "    cm   = confusion_matrix(y_true, y_pred)\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec  = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1   = f1_score(y_true, y_pred)\n",
    "    auc  = roc_auc_score(y_true, y_prob)\n",
    "    return cm, acc, prec, rec, f1, auc\n",
    "\n",
    "train_cm, train_acc, train_prec, train_rec, train_f1, train_auc = evaluate(train_y, train_p, train_c)\n",
    "val_cm,   val_acc,   val_prec,   val_rec,   val_f1,   val_auc   = evaluate(val_y,   val_p,   val_c)\n",
    "\n",
    "print(\"\\nTRAIN METRICS\")\n",
    "print(f\"Acc {train_acc:.3f} | Prec {train_prec:.3f} | Rec {train_rec:.3f} | F1 {train_f1:.3f} | AUC {train_auc:.3f}\")\n",
    "print(\"Confusion Matrix:\\n\", train_cm)\n",
    "\n",
    "print(\"\\nVAL METRICS\")\n",
    "print(f\"Acc {val_acc:.3f} | Prec {val_prec:.3f} | Rec {val_rec:.3f} | F1 {val_f1:.3f} | AUC {val_auc:.3f}\")\n",
    "print(\"Confusion Matrix:\\n\", val_cm)\n",
    "\n",
    "# --- visualize confusion matrices ---\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,4))\n",
    "sns.heatmap(train_cm, annot=True, fmt='d', cmap='Blues', ax=ax[0])\n",
    "ax[0].set_title('Train Confusion Matrix'); ax[0].set_xlabel('Predicted'); ax[0].set_ylabel('Actual')\n",
    "\n",
    "sns.heatmap(val_cm, annot=True, fmt='d', cmap='Blues', ax=ax[1])\n",
    "ax[1].set_title('Validation Confusion Matrix'); ax[1].set_xlabel('Predicted'); ax[1].set_ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5b58ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5df511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DISCRETIZATION: Convert continuous discharge into wet/dry ---\n",
    "DRY_THRESHOLD = 0.00014   # per Jake's message\n",
    "\n",
    "# Create a new binary column from discharge\n",
    "merged_df[\"wetdry_discharge\"] = (merged_df[\"Discharge_CMS\"] >= DRY_THRESHOLD).astype(int)\n",
    "\n",
    "print(\"Wet/dry (from discharge) distribution:\")\n",
    "print(merged_df[\"wetdry_discharge\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6672737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Combine HOBO + discharge discretization ---\n",
    "merged_df[\"wetdry_final\"] = merged_df[\"HoboWetDry0.05\"]\n",
    "\n",
    "# Fill missing HOBO with discharge discretization\n",
    "merged_df[\"wetdry_final\"] = merged_df[\"wetdry_final\"].fillna(merged_df[\"wetdry_discharge\"])\n",
    "\n",
    "print(\"Final combined wet/dry distribution:\")\n",
    "print(merged_df[\"wetdry_final\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f51a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"wetdry_final\"\n",
    "\n",
    "feature_cols = [c for c in merged_df.columns\n",
    "                if c not in [\"Date\", \"NHDPlusID\", target_col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5775c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create discharge-based wet/dry\n",
    "merged_df[\"wetdry_discharge\"] = (merged_df[\"Discharge_CMS\"] >= DRY_THRESHOLD).astype(int)\n",
    "\n",
    "# Combine HOBO + discharge discretization\n",
    "merged_df[\"wetdry_final\"] = merged_df[\"HoboWetDry0.05\"]\n",
    "merged_df[\"wetdry_final\"] = merged_df[\"wetdry_final\"].fillna(merged_df[\"wetdry_discharge\"])\n",
    "\n",
    "print(\"Final wet/dry distribution:\")\n",
    "print(merged_df[\"wetdry_final\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a99422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the sites that have both classes\n",
    "valid_obs = merged_df.dropna(subset=[\"wetdry_final\"])\n",
    "site_variation = valid_obs.groupby(\"NHDPlusID\")[\"wetdry_final\"].nunique()\n",
    "sites_with_both = site_variation[site_variation > 1].index.tolist()\n",
    "\n",
    "# pick one or all sites\n",
    "site_df = merged_df[merged_df[\"NHDPlusID\"].isin(sites_with_both)].sort_values(\"Date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f960bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = site_df.copy()\n",
    "\n",
    "# 1. Fill all NaNs per feature\n",
    "df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# 2. If any remain (because entire column missing early)\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2165993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "# 1. Keep numeric only\n",
    "df_numeric = df.select_dtypes(include=[np.number])\n",
    "\n",
    "print(\"Dropped columns:\", set(df.columns) - set(df_numeric.columns))\n",
    "\n",
    "# 2. Scale\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(df_numeric.values)\n",
    "df_scaled = pd.DataFrame(scaled, columns=df_numeric.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d4832",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"wetdry_final\"  # adjust to correct name\n",
    "\n",
    "y = df[target_col].values\n",
    "df = df.drop(columns=[target_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf109c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df.select_dtypes(include=[np.number])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75224d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df, #can change seq length to be 60 to\n",
    "                     seq_len=30, target_col=\"wetdry_final\"):\n",
    "    X, y = [], []\n",
    "    values = df.values\n",
    "    labels = df[target_col].values\n",
    "\n",
    "    for i in range(len(df) - seq_len):\n",
    "        seq = values[i:i+seq_len]\n",
    "        target = labels[i+seq_len]\n",
    "\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa0c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"wetdry_final\"  # CHANGE THIS IF NEEDED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01e8044",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_original = site_df[target_col].astype(int).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecddc0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled_with_label = df_scaled.copy()\n",
    "df_scaled_with_label[target_col] = y_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75688460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#can change sequence length here too\n",
    "X, y = create_sequences(site_df, seq_len=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc89931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# 1. Clean site_df\n",
    "# ===========================================\n",
    "df = site_df.copy()\n",
    "df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "\n",
    "target_col = \"wetdry_final\"     # <----- your label\n",
    "y_original = df[target_col].astype(int).values\n",
    "\n",
    "# ===========================================\n",
    "# 2. Select numeric features only (remove date)\n",
    "# ===========================================\n",
    "df_numeric = df.select_dtypes(include=[np.number]).drop(columns=[target_col], errors='ignore')\n",
    "\n",
    "# ===========================================\n",
    "# 3. Scale features\n",
    "# ===========================================\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(df_numeric)\n",
    "df_scaled = pd.DataFrame(scaled, columns=df_numeric.columns)\n",
    "\n",
    "# ===========================================\n",
    "# 4. Add label back\n",
    "# ===========================================\n",
    "df_scaled_with_label = df_scaled.copy()\n",
    "df_scaled_with_label[target_col] = y_original\n",
    "\n",
    "print(\"df_scaled_with_label shape:\", df_scaled_with_label.shape)\n",
    "\n",
    "# ===========================================\n",
    "# 5. Build sequences\n",
    "# ===========================================\n",
    "X, y = create_sequences(df_scaled_with_label, seq_len=30, target_col=target_col)\n",
    "\n",
    "print(\"NaNs in X:\", np.isnan(X).sum())\n",
    "print(\"NaNs in y:\", np.isnan(y).sum())\n",
    "print(\"Shapes:\", X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715cbaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea9cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADASYN for class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35679d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cedcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, T, d = X.shape\n",
    "\n",
    "# Flatten sequences: (n, T, d) -> (n, T*d)\n",
    "X_flat = X.reshape(n, T * d)\n",
    "\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_flat_res, y_res = adasyn.fit_resample(X_flat, y.astype(int))\n",
    "\n",
    "# Reshape back to (n_resampled, T, d)\n",
    "X_res = X_flat_res.reshape(-1, T, d)\n",
    "\n",
    "print(\"Original class counts:\", np.bincount(y.astype(int)))\n",
    "print(\"After ADASYN:\", np.bincount(y_res.astype(int)))\n",
    "print(\"Original shape:\", X.shape, \"Resampled shape:\", X_res.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4fe40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# =======================\n",
    "# Convert to tensors (use ADASYN output)\n",
    "# =======================\n",
    "X_tensor = torch.tensor(X_res, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_res.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "print(\"X_tensor:\", X_tensor.shape)\n",
    "print(\"y_tensor:\", y_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99067647",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# =======================\n",
    "# Train/Val split\n",
    "# =======================\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_tensor, y_tensor, test_size=0.2, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape)\n",
    "\n",
    "# =======================\n",
    "# Compute pos_weight safely\n",
    "# =======================\n",
    "pos = int(y_train.sum().item())\n",
    "neg = int((1 - y_train).sum().item())\n",
    "safe_pos_weight = 1.0  # KEEP MILD FOR NOW\n",
    "\n",
    "print(\"safe_pos_weight =\", safe_pos_weight)\n",
    "pos_weight_tensor = torch.tensor([safe_pos_weight], dtype=torch.float32)\n",
    "\n",
    "# =======================\n",
    "# Define LSTM Model\n",
    "# =======================\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=48, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        logits = self.fc(out[:, -1, :])\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = LSTMModel(X_tensor.shape[2])\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(X_val, y_val), batch_size=32, shuffle=False)\n",
    "\n",
    "# =======================\n",
    "# Train with early stopping\n",
    "# =======================\n",
    "best_val_loss = float(\"inf\")\n",
    "best_state = None\n",
    "patience = 3\n",
    "wait = 0\n",
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(xb)\n",
    "        logits = torch.nan_to_num(logits, nan=0.0, posinf=5.0, neginf=-5.0)\n",
    "\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # ---- validation loss ----\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            logits = model(xb)\n",
    "            logits = torch.nan_to_num(logits, nan=0.0, posinf=5.0, neginf=-5.0)\n",
    "            val_loss += criterion(logits, yb).item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss={total_loss/len(train_loader):.4f} | Val Loss={val_loss/len(val_loader):.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state = model.state_dict().copy()\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "print(\"Loaded best model state.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8927de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Predict safely\n",
    "# =======================\n",
    "def get_probs(loader):\n",
    "    model.eval()\n",
    "    preds, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            logits = model(xb)\n",
    "            logits = torch.nan_to_num(logits, nan=0.0)\n",
    "\n",
    "            probs = torch.sigmoid(logits).squeeze().cpu().numpy()\n",
    "            probs = np.nan_to_num(probs, nan=0.0)\n",
    "\n",
    "            preds.append(probs)\n",
    "            actuals.append(yb.squeeze().cpu().numpy())\n",
    "\n",
    "    return np.concatenate(preds), np.concatenate(actuals)\n",
    "\n",
    "train_probs, train_true = get_probs(train_loader)\n",
    "val_probs, val_true = get_probs(val_loader)\n",
    "\n",
    "# =======================\n",
    "# Apply threshold\n",
    "# =======================\n",
    "threshold = 0.5\n",
    "train_pred = (train_probs >= threshold).astype(int)\n",
    "val_pred = (val_probs >= threshold).astype(int)\n",
    "\n",
    "# =======================\n",
    "# Compute metrics\n",
    "# =======================\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def compute_metrics(y_true, y_prob, y_pred):\n",
    "    return (\n",
    "        confusion_matrix(y_true, y_pred),\n",
    "        accuracy_score(y_true, y_pred),\n",
    "        precision_score(y_true, y_pred, zero_division=0),\n",
    "        recall_score(y_true, y_pred, zero_division=0),\n",
    "        f1_score(y_true, y_pred),\n",
    "        roc_auc_score(y_true, y_prob)\n",
    "    )\n",
    "\n",
    "train_cm, train_acc, train_prec, train_rec, train_f1, train_auc = compute_metrics(train_true, train_probs, train_pred)\n",
    "val_cm,   val_acc,   val_prec,   val_rec,   val_f1,   val_auc   = compute_metrics(val_true, val_probs, val_pred)\n",
    "\n",
    "print(\"\\nTRAIN METRICS\")\n",
    "print(train_cm, train_acc, train_prec, train_rec, train_f1, train_auc)\n",
    "\n",
    "print(\"\\nVAL METRICS\")\n",
    "print(val_cm, val_acc, val_prec, val_rec, val_f1, val_auc)\n",
    "\n",
    "# =======================\n",
    "# Plot confusion matrices\n",
    "# =======================\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.heatmap(train_cm, annot=True, fmt='d', cmap='Blues', ax=ax[0])\n",
    "ax[0].set_title(\"Train Confusion Matrix\")\n",
    "\n",
    "sns.heatmap(val_cm, annot=True, fmt='d', cmap='Blues', ax=ax[1])\n",
    "ax[1].set_title(\"Validation Confusion Matrix\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fc19e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_score,\n",
    "    recall_score, f1_score, roc_auc_score\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1. GET PREDICTIONS\n",
    "# ---------------------------------------------------\n",
    "train_probs, train_true = get_probs(train_loader)\n",
    "val_probs,   val_true   = get_probs(val_loader)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. SANITIZE ALL VALUES (critical)\n",
    "# ---------------------------------------------------\n",
    "train_probs = np.nan_to_num(train_probs, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "val_probs   = np.nan_to_num(val_probs,   nan=0.0, posinf=1.0, neginf=0.0)\n",
    "\n",
    "train_true = np.nan_to_num(train_true, nan=0.0)\n",
    "val_true   = np.nan_to_num(val_true,   nan=0.0)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. CHOOSE THRESHOLD\n",
    "# ---------------------------------------------------\n",
    "threshold = 0.5  # you can swap in your tuned threshold\n",
    "\n",
    "train_pred = (train_probs >= threshold).astype(int)\n",
    "val_pred   = (val_probs   >= threshold).astype(int)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4. EVALUATION FUNCTION\n",
    "# ---------------------------------------------------\n",
    "def compute_metrics(y_true, y_prob, y_pred):\n",
    "    cm   = confusion_matrix(y_true, y_pred)\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec  = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1   = f1_score(y_true, y_pred)\n",
    "    auc  = roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else float('nan')\n",
    "    return cm, acc, prec, rec, f1, auc\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5. COMPUTE TRAIN + VAL METRICS\n",
    "# ---------------------------------------------------\n",
    "train_cm, train_acc, train_prec, train_rec, train_f1, train_auc = \\\n",
    "    compute_metrics(train_true, train_probs, train_pred)\n",
    "\n",
    "val_cm, val_acc, val_prec, val_rec, val_f1, val_auc = \\\n",
    "    compute_metrics(val_true, val_probs, val_pred)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 6. PRINT METRICS\n",
    "# ---------------------------------------------------\n",
    "print(\"\\n================ TRAIN METRICS ================\")\n",
    "print(f\"Accuracy:  {train_acc:.4f}\")\n",
    "print(f\"Precision: {train_prec:.4f}\")\n",
    "print(f\"Recall:    {train_rec:.4f}\")\n",
    "print(f\"F1 Score:  {train_f1:.4f}\")\n",
    "print(f\"AUC:       {train_auc:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", train_cm)\n",
    "\n",
    "print(\"\\n================ VAL METRICS ================\")\n",
    "print(f\"Accuracy:  {val_acc:.4f}\")\n",
    "print(f\"Precision: {val_prec:.4f}\")\n",
    "print(f\"Recall:    {val_rec:.4f}\")\n",
    "print(f\"F1 Score:  {val_f1:.4f}\")\n",
    "print(f\"AUC:       {val_auc:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", val_cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d00271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MISC Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c832ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Only numeric columns\n",
    "numeric_df = df_scaled_with_label.select_dtypes(include=[np.number])\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(numeric_df.corr(), cmap='coolwarm', center=0, annot=False)\n",
    "plt.title(\"Correlation Heatmap of All Numeric Features\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e14d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_target = numeric_df.corr()[target_col].sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(6,10))\n",
    "sns.barplot(y=corr_target.index, x=corr_target.values, palette=\"viridis\")\n",
    "plt.title(\"Correlation of Features with Wet/Dry Target\")\n",
    "plt.xlabel(\"Correlation\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e5ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = site_df.copy()\n",
    "df = df.sort_values(\"Date\")\n",
    "\n",
    "df[\"discharge_lag1\"] = df[\"Discharge_CMS\"].shift(1)\n",
    "df[\"discharge_lag7\"] = df[\"Discharge_CMS\"].shift(7)\n",
    "df[\"discharge_lag30\"] = df[\"Discharge_CMS\"].shift(30)\n",
    "\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(1,3,1)\n",
    "sns.scatterplot(x=df[\"discharge_lag1\"], y=df[\"Discharge_CMS\"], s=10)\n",
    "plt.title(\"Discharge vs Lag-1\")\n",
    "plt.xlabel(\"Lag 1\")\n",
    "plt.ylabel(\"Current\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.scatterplot(x=df[\"discharge_lag7\"], y=df[\"Discharge_CMS\"], s=10)\n",
    "plt.title(\"Discharge vs Lag-7\")\n",
    "plt.xlabel(\"Lag 7\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.scatterplot(x=df[\"discharge_lag30\"], y=df[\"Discharge_CMS\"], s=10)\n",
    "plt.title(\"Discharge vs Lag-30\")\n",
    "plt.xlabel(\"Lag 30\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f8dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = site_df.copy().sort_values(\"Date\")\n",
    "\n",
    "df[\"rolling_wet\"] = df[target_col].rolling(100).mean()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(df[\"Date\"], df[\"rolling_wet\"])\n",
    "plt.title(\"Rolling % of Wet Days (Window=100)\")\n",
    "plt.ylabel(\"Pct Wet\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422e2da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = merged_df.groupby(\"NHDPlusID\")[target_col].mean().sort_values()\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "sns.barplot(x=counts.index.astype(str), y=counts.values, palette=\"plasma\")\n",
    "plt.xticks([], [])  # hide long site IDs\n",
    "plt.title(\"Fraction of Wet Observations per Site\")\n",
    "plt.ylabel(\"Pct Wet\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c0ae26",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[0;32m/tmp/ipython-input-1569227641.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 20\u001b[0;31m \u001b[0mimps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperm_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     22\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def perm_importance(model, X_val, y_val, n_repeats=5):\n",
    "    base_preds = torch.sigmoid(model(X_val)).detach().cpu().numpy()\n",
    "    base_f1 = f1_score(y_val, (base_preds>0.5))\n",
    "\n",
    "    importances = []\n",
    "\n",
    "    for col in range(X_val.shape[2]):\n",
    "        f1_scores = []\n",
    "        for _ in range(n_repeats):\n",
    "            X_permuted = X_val.clone()\n",
    "            X_permuted[:,:,col] = X_val[:,:,col][torch.randperm(X_val.shape[0])]\n",
    "            perm_preds = torch.sigmoid(model(X_permuted)).detach().cpu().numpy()\n",
    "            f1_scores.append(f1_score(y_val, (perm_preds>0.5)))\n",
    "        importances.append(base_f1 - np.mean(f1_scores))\n",
    "\n",
    "    return np.array(importances)\n",
    "\n",
    "imps = perm_importance(model, X_val, y_val.squeeze().numpy())\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(x=np.arange(len(imps)), y=imps)\n",
    "plt.title(\"Permutation Importance per Feature Index\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Importance (F1 Drop)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bf34f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 158603\n",
      "Date range: 1980-01-01 → 2023-06-26\n",
      "\n",
      "Target balance (wetdry_discharge):\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'wetdry_discharge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n",
      "\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\n",
      "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\n",
      "\u001b[0;31mKeyError\u001b[0m: 'wetdry_discharge'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)\n",
      "\u001b[0;32m/tmp/ipython-input-1567030871.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTarget balance (wetdry_discharge):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wetdry_discharge\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      8\u001b[0m \u001b[0;31m#make sure no nulls and stuff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n",
      "\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n",
      "\u001b[1;32m   3810\u001b[0m             ):\n",
      "\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;31mKeyError\u001b[0m: 'wetdry_discharge'"
     ]
    }
   ],
   "source": [
    "# some quick sanity checks before more modeling\n",
    "print(\"Rows:\", len(merged_df))\n",
    "print(\"Date range:\", merged_df[\"Date\"].min(), \"→\", merged_df[\"Date\"].max())\n",
    "\n",
    "print(\"\\nTarget balance (wetdry_discharge):\")\n",
    "print(merged_df[\"wetdry_discharge\"].value_counts())\n",
    "\n",
    "#make sure no nulls and stuff\n",
    "num_cols = merged_df.select_dtypes(include=[np.number]).columns\n",
    "print(\"\\nNaNs in numeric cols:\", merged_df[num_cols].isna().sum().sum())\n",
    "print(\"Infs in numeric cols:\", np.isinf(merged_df[num_cols]).sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f218af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation with target\n",
    "corr = merged_df[num_cols].corr()[\"wetdry_discharge\"].sort_values()\n",
    "plt.figure(figsize=(6, 4))\n",
    "corr.drop(\"wetdry_discharge\").plot(kind=\"barh\")\n",
    "plt.title(\"Correlation with wetdry_discharge\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc12bf97",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[0;32m/tmp/ipython-input-2116446373.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m      2\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Discharge_lag1\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NHDPlusID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Discharge_CMS\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Discharge_CMS\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Discharge_lag1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Discharge (today)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# discharge vs lagged discharge\n",
    "merged_df[\"Discharge_lag1\"] = merged_df.groupby(\"NHDPlusID\")[\"Discharge_CMS\"].shift(1)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(merged_df[\"Discharge_CMS\"],merged_df[\"Discharge_lag1\"],s=3,alpha=0.3)\n",
    "plt.xlabel(\"Discharge (today)\")\n",
    "plt.ylabel(\"Discharge (lag 1)\")\n",
    "plt.title(\"Discharge persistence\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9205338a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label counts:\n",
      "1.0    12682\n",
      "Name: count, dtype: int64\n",
      "pos_weight used = 0.00\n"
     ]
    }
   ],
   "source": [
    "# class imbalance summary(train split)\n",
    "print(\"Train label counts:\")\n",
    "print(pd.Series(y_train.flatten()).value_counts())\n",
    "# pos_weight used in BceWithLogitsLoss\n",
    "pos = (y_train == 1).sum()\n",
    "neg = (y_train == 0).sum()\n",
    "print(f\"pos_weight used = {neg / max(pos,1):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2f44a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
