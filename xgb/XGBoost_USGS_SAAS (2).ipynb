{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Make necessary imports\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKe8RuQuKYvt",
        "outputId": "30dd71cf-84d4-4d52-f5ef-428d9912dc53"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "JkTVZxXqjSho"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load drivers dataset as CSV\n",
        "drivers = pd.read_csv('/content/drive/MyDrive/drivers (2).csv')"
      ],
      "metadata": {
        "id": "dXrt7iR4UyCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drivers"
      ],
      "metadata": {
        "id": "3AEBrldWjlDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loads obs data\n",
        "data = pd.read_parquet('obs.parquet')"
      ],
      "metadata": {
        "id": "qzrsy95_j6FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "5Xsja-1kkiVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Isolate the target variable by creating dictionary of all metrics and choosing 'value'\n",
        "dfs = {val: data[data['variable'] == val ]for val in data['variable'].unique()}\n",
        "\n",
        "wet_dry_df = dfs['HoboWetDry0.05']"
      ],
      "metadata": {
        "id": "GBfuLMadkjen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load in remaining datasets\n",
        "static_df = pd.read_parquet('static_vars.parquet')\n",
        "degrees = pd.read_parquet('degrees.parquet')"
      ],
      "metadata": {
        "id": "B0odQTENzbV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make sure all NHD IDs are strings, normalize before merging\n",
        "wet_dry_df['NHDPlusID'] = wet_dry_df['NHDPlusID'].astype(str)\n",
        "drivers['NHDPlusID'] = drivers['NHDPlusID'].astype(str)\n",
        "static_df['NHDPlusID'] = static_df['NHDPlusID'].astype(str)\n",
        "\n",
        "#Make sure all date values are in datetime format, normalize before merging\n",
        "wet_dry_df['Date'] = pd.to_datetime(wet_dry_df['Date'])\n",
        "drivers['Date'] = pd.to_datetime(drivers['Date'])\n",
        "\n",
        "static_df_pivot = static_df.pivot(\n",
        "    index='NHDPlusID',\n",
        "    columns='variable',\n",
        "    values='value'\n",
        ").reset_index()\n",
        "\n",
        "#Merge all datasets to create one central dataset\n",
        "central_df = wet_dry_df.merge(drivers, on=['NHDPlusID', 'Date'], how='inner')\n",
        "central_df = central_df.merge(static_df_pivot, on='NHDPlusID', how='left')\n",
        "central_df = central_df.merge(degrees, on='NHDPlusID', how='left')"
      ],
      "metadata": {
        "id": "A3E9v8ler9aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sort by date within each NHDPlusID\n",
        "central_df = central_df.sort_values(['NHDPlusID', 'Date'])\n",
        "\n",
        "#Introduce lag by shifting the label back\n",
        "#EXPERIMENT WITH THIS VALUE (which is currently -7)\n",
        "central_df['wet_dry_next'] = central_df.groupby('NHDPlusID')['value'].shift(-7)\n",
        "\n",
        "#Drop all null values\n",
        "central_df = central_df.dropna(subset = 'wet_dry_next')"
      ],
      "metadata": {
        "id": "NKDmH7WwMFqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Random Splitting Approach**"
      ],
      "metadata": {
        "id": "8v9TV9Wa7lD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Features and target\n",
        "X = central_df.drop('wet_dry_next', axis=1)\n",
        "y = central_df['wet_dry_next']\n",
        "\n",
        "#Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "8xDbCryw-PZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Temporal Splitting Approach**"
      ],
      "metadata": {
        "id": "mbAwFCa552b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sort by time\n",
        "df = central_df.sort_values(['Date'])\n",
        "split_date = '2020-9-15'\n",
        "\n",
        "#Introduce temporal based splitting\n",
        "#Every entry before 9/15 is training; rest is testing\n",
        "train = df[df['Date'] < split_date]\n",
        "test = df[df['Date'] >= split_date]\n",
        "\n",
        "X_train = train.drop('wet_dry_next', axis=1)\n",
        "y_train = train['wet_dry_next']\n",
        "X_test = test.drop('wet_dry_next', axis=1)\n",
        "y_test = test['wet_dry_next']"
      ],
      "metadata": {
        "id": "DOy943YN5r9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Site-based Splitting Approach**"
      ],
      "metadata": {
        "id": "-mnTqBBO5cYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sort data chronologically\n",
        "central_df = central_df.sort_values(['Date']).reset_index(drop=True)\n",
        "\n",
        "#Split sites into train and test groups\n",
        "sites = central_df['SiteIDCode'].unique()\n",
        "train_sites = sites[:int(0.8 * len(sites))]\n",
        "test_sites  = sites[int(0.8 * len(sites)):]\n",
        "\n",
        "#Create site-based train and test sets\n",
        "train = central_df[central_df['SiteIDCode'].isin(train_sites)].copy()\n",
        "test  = central_df[central_df['SiteIDCode'].isin(test_sites)].copy()\n",
        "\n",
        "#Features and target\n",
        "X_train = train.drop('wet_dry_next', axis=1)\n",
        "y_train = train['wet_dry_next']\n",
        "X_test = test.drop('wet_dry_next', axis=1)\n",
        "y_test = test['wet_dry_next']"
      ],
      "metadata": {
        "id": "SE_GZYLBPx2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop non-numerical, non-influential columns\n",
        "X_train = X_train.drop(['variable', 'NHDPlusID',\t'SiteIDCode',\t'Date'], axis = 1)\n",
        "X_test = X_test.drop(['variable', 'NHDPlusID',\t'SiteIDCode',\t'Date'], axis = 1)"
      ],
      "metadata": {
        "id": "sNK0ml5e51Wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply SMOTE to fix class imbalance\n",
        "sm = SMOTE(random_state=42, k_neighbors=5)\n",
        "X_train, y_train = sm.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "5VTNboq8Kf6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define XGBoost model and fit it on training data\n",
        "XGB_model = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=3,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "XGB_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "K5-n_Y2KRz0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply model onto test data and save predictions\n",
        "y_pred = XGB_model.predict(X_test)\n",
        "\n",
        "#Obtain performance metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "roc = roc_auc_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "wFxKdcjJTRm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy"
      ],
      "metadata": {
        "id": "-7_dqtq7TcPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roc"
      ],
      "metadata": {
        "id": "QMp219j-Tc6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Dry', 'Wet'])\n",
        "disp.plot(cmap='Blues', values_format='d')\n",
        "\n",
        "plt.title(\"Confusion Matrix - Site-based splitting, No lag\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YKtSt6gMUlhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make feature importances table\n",
        "feature_cols = X_train.columns.tolist()\n",
        "\n",
        "importances = pd.DataFrame({\n",
        "    \"Feature\": feature_cols,\n",
        "    \"Importance\": XGB_model.feature_importances_\n",
        "})\n",
        "\n",
        "print(importances)\n"
      ],
      "metadata": {
        "id": "-0NFf-dvOB7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define function to make predictions given site and date\n",
        "FEATURE_COLS = list(X_train.columns)\n",
        "def predict_site_date(model, central_df, site_id, date):\n",
        "  date = pd.to_datetime(date)\n",
        "  row = central_df[(central_df[\"SiteIDCode\"] == site_id) & (central_df[\"Date\"] == date)]\n",
        "  Xq = row[FEATURE_COLS]\n",
        "  pred_class = model.predict(Xq)\n",
        "  pred_prob = model.predict_proba(Xq)[:, 1]\n",
        "\n",
        "  return pd.DataFrame({\n",
        "      \"SiteIDCode\": row[\"SiteIDCode\"].values,\n",
        "      \"Date\": row[\"Date\"].values,\n",
        "      \"pred_class\": pred_class,\n",
        "      \"pred_prob\": pred_prob\n",
        "  })"
      ],
      "metadata": {
        "id": "3pS9h0K2O7YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_site_date(\n",
        "    model=XGB_model,\n",
        "    central_df=central_df,\n",
        "    site_id=\"01137500\",\n",
        "    date=\"2020-09-15\"\n",
        ")"
      ],
      "metadata": {
        "id": "NLOnsURGjfSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BQCeVMEFjjIo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}